{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home')\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore', module='skimage')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from kerasgym.models import cnn_model_base, dense_model_base, DDPGModel, DQNModel\n",
    "from kerasgym.agents import Agent\n",
    "from kerasgym.agents.process_state import downsample, rgb_to_binary\n",
    "from kerasgym.agents.process_state import stack_consecutive, combine_consecutive\n",
    "from kerasgym.agents.process_prediction import argmax_scalar, scalar_to_onehot\n",
    "from kerasgym.agents.exploration import LinearDecay, ScopingPeriodic, EpsilonGreedy\n",
    "from kerasgym.agents.exploration import graph_schedule\n",
    "from keras.optimizers import RMSprop, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: split agents into discrete and continuous, do cleanup from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode: 0. Average Reward: 6.5. Average Duration: 6.5. Explore: 0.998\n",
      "Episode: 10. Average Reward: 17.1. Average Duration: 17.1. Explore: 0.98\n",
      "Episode: 20. Average Reward: 21.4. Average Duration: 21.4. Explore: 0.962\n",
      "Episode: 30. Average Reward: 27.1. Average Duration: 27.1. Explore: 0.944\n",
      "Episode: 40. Average Reward: 18.9. Average Duration: 18.9. Explore: 0.926\n",
      "Episode: 50. Average Reward: 23.3. Average Duration: 23.3. Explore: 0.908\n",
      "Episode: 60. Average Reward: 29.2. Average Duration: 29.2. Explore: 0.89\n",
      "Episode: 70. Average Reward: 24.0. Average Duration: 24.0. Explore: 0.872\n",
      "Episode: 80. Average Reward: 22.7. Average Duration: 22.7. Explore: 0.854\n",
      "Episode: 90. Average Reward: 41.3. Average Duration: 41.3. Explore: 0.836\n",
      "Episode: 100. Average Reward: 27.9. Average Duration: 27.9. Explore: 0.818\n",
      "Episode: 110. Average Reward: 38.4. Average Duration: 38.4. Explore: 0.8\n",
      "Episode: 120. Average Reward: 34.9. Average Duration: 34.9. Explore: 0.782\n",
      "Episode: 130. Average Reward: 35.4. Average Duration: 35.4. Explore: 0.764\n",
      "Episode: 140. Average Reward: 51.9. Average Duration: 51.9. Explore: 0.746\n",
      "Episode: 150. Average Reward: 46.9. Average Duration: 46.9. Explore: 0.728\n",
      "Episode: 160. Average Reward: 35.0. Average Duration: 35.0. Explore: 0.71\n",
      "Episode: 170. Average Reward: 54.7. Average Duration: 54.7. Explore: 0.692\n",
      "Episode: 180. Average Reward: 61.2. Average Duration: 61.2. Explore: 0.674\n",
      "Episode: 190. Average Reward: 44.1. Average Duration: 44.1. Explore: 0.656\n",
      "Episode: 200. Average Reward: 60.7. Average Duration: 60.7. Explore: 0.638\n",
      "Episode: 210. Average Reward: 55.4. Average Duration: 55.4. Explore: 0.62\n",
      "Episode: 220. Average Reward: 77.9. Average Duration: 77.9. Explore: 0.602\n",
      "Episode: 230. Average Reward: 113.9. Average Duration: 113.9. Explore: 0.584\n",
      "Episode: 240. Average Reward: 80.9. Average Duration: 80.9. Explore: 0.566\n",
      "Episode: 250. Average Reward: 143.4. Average Duration: 143.4. Explore: 0.548\n",
      "Episode: 260. Average Reward: 114.0. Average Duration: 114.0. Explore: 0.53\n",
      "Episode: 270. Average Reward: 196.6. Average Duration: 196.6. Explore: 0.512\n",
      "Episode: 280. Average Reward: 149.5. Average Duration: 149.5. Explore: 0.494\n",
      "Episode: 290. Average Reward: 178.1. Average Duration: 178.1. Explore: 0.476\n",
      "Episode: 300. Average Reward: 196.9. Average Duration: 196.9. Explore: 0.458\n",
      "Episode: 310. Average Reward: 219.4. Average Duration: 219.4. Explore: 0.44\n"
     ]
    }
   ],
   "source": [
    "# CARTPOLE\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "\n",
    "base_config = {\n",
    "    'in_shape': env.observation_space.shape,\n",
    "    'layer_sizes': [64, 32, 16],\n",
    "    'activation': 'relu'\n",
    "}\n",
    "\n",
    "base_model = dense_model_base(**base_config)\n",
    "\n",
    "dqn_config = {\n",
    "    'action_dim': env.action_space.n,\n",
    "    'gamma': 0.99,\n",
    "    'tau': 1.0,\n",
    "    #'optimizer': RMSprop(lr=0.0025, rho=0.95, epsilon=0.01)\n",
    "    'optimizer': Adam(lr=0.001)\n",
    "}\n",
    "model = DQNModel(base_model, **dqn_config)\n",
    "\n",
    "schedule = LinearDecay(1.0, 0.1, 500, -1)\n",
    "explorer = EpsilonGreedy(schedule)\n",
    "buffer_size = 10000\n",
    "batch_size = 32\n",
    "\n",
    "agent = Agent(env,\n",
    "              state_processing_fns=[],\n",
    "              model=model, ptoc_fn=argmax_scalar(),\n",
    "              ctol_fn=scalar_to_onehot(),\n",
    "              explorer=explorer, buffer_size=buffer_size,\n",
    "              batch_size=batch_size, warmup_length=0)\n",
    "agent.reset()\n",
    "agent.run_indefinitely()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOUNTAIN CAR\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env.reset()\n",
    "\n",
    "base_config = {\n",
    "    'in_shape': env.observation_space.shape,\n",
    "    'layer_sizes': [16, 16],\n",
    "    'activation': 'relu'\n",
    "}\n",
    "\n",
    "base_model = dense_model_base(**base_config)\n",
    "\n",
    "ddpg_config = {\n",
    "    'action_dim': env.ac`tion_space.shape[0],\n",
    "    'actor_activation': 'softmax',\n",
    "    'gamma': 0.99,\n",
    "    'tau': 0.125,\n",
    "    'actor_alpha': 1e-3,\n",
    "    'critic_alpha': 1e-3\n",
    "}\n",
    "model = DDPGModel(base_model, **ddpg_config)\n",
    "\n",
    "schedule = LinearDecay(1.0, 0.1, 500, -1)\n",
    "explorer = EpsilonGreedy(schedule, discrete=False)\n",
    "buffer_size = 10000\n",
    "batch_size = 32\n",
    "\n",
    "agent = Agent(env,\n",
    "              state_processing_fns=[],\n",
    "              model=model, ptoc_fn=lambda x,y: return x,\n",
    "              ctol_fn=lambda x,y: return x,\n",
    "              explorer=explorer, buffer_size=buffer_size,\n",
    "              batch_size=batch_size, warmup_length=0)\n",
    "agent.reset()\n",
    "agent.run_indefinitely()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BREAKOUT\n",
    "\n",
    "# env\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env.reset()\n",
    "\n",
    "# custom shape due to downsampling and stacking\n",
    "shape = (105, 80, 4)\n",
    "\n",
    "# model\n",
    "base_config = {\n",
    "    'in_shape': shape,\n",
    "    'conv_layer_sizes': [16, 32],\n",
    "    'fc_layer_sizes': [256],\n",
    "    'kernel_sizes': [(8,8), (4,4)],\n",
    "    'strides': [(4,4), (2,2)],\n",
    "    'activation': 'relu'\n",
    "}\n",
    "\n",
    "base_model = cnn_model_base(**base_config)\n",
    "\n",
    "dqn_config = {\n",
    "    'action_dim': env.action_space.n,\n",
    "    'gamma': 0.99,\n",
    "    'tau': 0.15,\n",
    "    'optimizer': RMSprop(lr=0.00025, rho=0.95, epsilon=0.01)\n",
    "}\n",
    "model = DQNModel(base_model, **dqn_config)\n",
    "\n",
    "schedule = LinearDecay(1.0, 0.1, 1000000, interval=1)\n",
    "explorer = EpsilonGreedy(schedule)\n",
    "buffer_size = 100000\n",
    "batch_size = 32\n",
    "\n",
    "agent = Agent(env,\n",
    "              state_processing_fns=[downsample(shape[:-1]), rgb_to_binary(),\n",
    "                                    #combine_consecutive(fun='diff'),\n",
    "                                    stack_consecutive(4)],\n",
    "              model=model, ptoc_fn=argmax_scalar(),\n",
    "              ctol_fn=scalar_to_onehot(),\n",
    "              explorer=explorer, buffer_size=buffer_size,\n",
    "              batch_size=batch_size, warmup_length=50000,\n",
    "              state_dtype=np.uint8)\n",
    "agent.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run_indefinitely()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
