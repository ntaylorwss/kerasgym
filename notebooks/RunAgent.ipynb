{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home')\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore', module='skimage')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from kerasgym.models import cnn_model_base, dense_model_base, DDPGModel, DQNModel\n",
    "from kerasgym.agents import Agent\n",
    "from kerasgym.agents.processing import downsample, rgb_to_binary\n",
    "from kerasgym.agents.processing import stack_consecutive, combine_consecutive\n",
    "from kerasgym.agents.schedules import LinearDecay, ScopingPeriodic\n",
    "from kerasgym.agents.actors import EpsilonGreedyActor\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "\n",
    "from kerasgym.agents.schedules import graph_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# CARTPOLE\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "\n",
    "base_config = {\n",
    "    'in_shape': env.observation_space.shape,\n",
    "    'layer_sizes': [64, 32, 16],\n",
    "    'activation': 'relu'\n",
    "}\n",
    "base_model = dense_model_base(**base_config)\n",
    "\n",
    "dqn_config = {\n",
    "    'gamma': 0.99,\n",
    "    'tau': 1.0,\n",
    "    'optimizer': Adam(lr=0.001)\n",
    "}\n",
    "model = DQNModel(base_model, **dqn_config)\n",
    "\n",
    "schedule = LinearDecay(1.0, 0.1, 500, -1)\n",
    "actor = EpsilonGreedyActor(epsilon_schedule=schedule)\n",
    "buffer_size = 10000\n",
    "batch_size = 32\n",
    "\n",
    "agent = Agent(env, state_pipeline=[], model=model, actor=actor, \n",
    "              buffer_size=buffer_size, batch_size=batch_size,\n",
    "              report_freq=10, warmup_length=0)\n",
    "agent.reset()\n",
    "#agent.run_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of episode 10. Last 10 episodes: Average reward: 17.7. Average duration: 17.7.\n",
      "End of episode 20. Last 10 episodes: Average reward: 20.6. Average duration: 20.6.\n",
      "End of episode 30. Last 10 episodes: Average reward: 22.2. Average duration: 22.2.\n",
      "End of episode 40. Last 10 episodes: Average reward: 19.4. Average duration: 19.4.\n",
      "End of episode 50. Last 10 episodes: Average reward: 28.7. Average duration: 28.7.\n",
      "End of episode 60. Last 10 episodes: Average reward: 19.6. Average duration: 19.6.\n",
      "End of episode 70. Last 10 episodes: Average reward: 23.4. Average duration: 23.4.\n",
      "End of episode 80. Last 10 episodes: Average reward: 29.2. Average duration: 29.2.\n",
      "End of episode 90. Last 10 episodes: Average reward: 20.6. Average duration: 20.6.\n",
      "End of episode 100. Last 10 episodes: Average reward: 23.1. Average duration: 23.1.\n",
      "End of episode 110. Last 10 episodes: Average reward: 27.5. Average duration: 27.5.\n",
      "End of episode 120. Last 10 episodes: Average reward: 28.4. Average duration: 28.4.\n",
      "End of episode 130. Last 10 episodes: Average reward: 42.1. Average duration: 42.1.\n",
      "End of episode 140. Last 10 episodes: Average reward: 34.6. Average duration: 34.6.\n",
      "End of episode 150. Last 10 episodes: Average reward: 28.1. Average duration: 28.1.\n",
      "End of episode 160. Last 10 episodes: Average reward: 57.8. Average duration: 57.8.\n",
      "End of episode 170. Last 10 episodes: Average reward: 38.3. Average duration: 38.3.\n",
      "End of episode 180. Last 10 episodes: Average reward: 23.1. Average duration: 23.1.\n",
      "End of episode 190. Last 10 episodes: Average reward: 53.7. Average duration: 53.7.\n",
      "End of episode 200. Last 10 episodes: Average reward: 55.3. Average duration: 55.3.\n",
      "End of episode 210. Last 10 episodes: Average reward: 43.7. Average duration: 43.7.\n",
      "End of episode 220. Last 10 episodes: Average reward: 68.5. Average duration: 68.5.\n",
      "End of episode 230. Last 10 episodes: Average reward: 63.6. Average duration: 63.6.\n",
      "End of episode 240. Last 10 episodes: Average reward: 83.5. Average duration: 83.5.\n",
      "End of episode 250. Last 10 episodes: Average reward: 123.6. Average duration: 123.6.\n",
      "End of episode 260. Last 10 episodes: Average reward: 121.0. Average duration: 121.0.\n",
      "End of episode 270. Last 10 episodes: Average reward: 118.6. Average duration: 118.6.\n",
      "End of episode 280. Last 10 episodes: Average reward: 135.9. Average duration: 135.9.\n",
      "End of episode 290. Last 10 episodes: Average reward: 171.6. Average duration: 171.6.\n",
      "End of episode 300. Last 10 episodes: Average reward: 136.5. Average duration: 136.5.\n",
      "End of episode 310. Last 10 episodes: Average reward: 129.7. Average duration: 129.7.\n",
      "End of episode 320. Last 10 episodes: Average reward: 185.9. Average duration: 185.9.\n",
      "End of episode 330. Last 10 episodes: Average reward: 125.3. Average duration: 125.3.\n",
      "End of episode 340. Last 10 episodes: Average reward: 161.3. Average duration: 161.3.\n",
      "End of episode 350. Last 10 episodes: Average reward: 152.0. Average duration: 152.0.\n",
      "End of episode 360. Last 10 episodes: Average reward: 138.9. Average duration: 138.9.\n",
      "End of episode 370. Last 10 episodes: Average reward: 186.7. Average duration: 186.7.\n",
      "End of episode 380. Last 10 episodes: Average reward: 195.5. Average duration: 195.5.\n",
      "End of episode 390. Last 10 episodes: Average reward: 196.8. Average duration: 196.8.\n",
      "End of episode 400. Last 10 episodes: Average reward: 164.7. Average duration: 164.7.\n",
      "End of episode 410. Last 10 episodes: Average reward: 11.0. Average duration: 11.0.\n",
      "End of episode 420. Last 10 episodes: Average reward: 132.0. Average duration: 132.0.\n",
      "End of episode 430. Last 10 episodes: Average reward: 92.1. Average duration: 92.1.\n",
      "End of episode 440. Last 10 episodes: Average reward: 57.8. Average duration: 57.8.\n",
      "End of episode 450. Last 10 episodes: Average reward: 65.8. Average duration: 65.8.\n",
      "End of episode 460. Last 10 episodes: Average reward: 61.4. Average duration: 61.4.\n",
      "End of episode 470. Last 10 episodes: Average reward: 71.8. Average duration: 71.8.\n",
      "End of episode 480. Last 10 episodes: Average reward: 69.0. Average duration: 69.0.\n",
      "End of episode 490. Last 10 episodes: Average reward: 48.1. Average duration: 48.1.\n",
      "End of episode 500. Last 10 episodes: Average reward: 9.1. Average duration: 9.1.\n",
      "End of episode 510. Last 10 episodes: Average reward: 11.9. Average duration: 11.9.\n",
      "End of episode 520. Last 10 episodes: Average reward: 9.4. Average duration: 9.4.\n",
      "End of episode 530. Last 10 episodes: Average reward: 8.9. Average duration: 8.9.\n",
      "End of episode 540. Last 10 episodes: Average reward: 9.8. Average duration: 9.8.\n",
      "End of episode 550. Last 10 episodes: Average reward: 106.3. Average duration: 106.3.\n",
      "Stopping.\n"
     ]
    }
   ],
   "source": [
    "agent.run_indefinitely()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = LinearDecay(0.9, 0.1, 0.0001)\n",
    "graph_schedule(s, 1000)\n",
    "s.t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ScopingPeriodic(0.9, 0.1, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3987268717780482"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s._periodic(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOUNTAIN CAR\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env.reset()\n",
    "\n",
    "base_config = {\n",
    "    'in_shape': env.observation_space.shape,\n",
    "    'layer_sizes': [16, 16],\n",
    "    'activation': 'relu'\n",
    "}\n",
    "\n",
    "base_model = dense_model_base(**base_config)\n",
    "\n",
    "ddpg_config = {\n",
    "    'action_dim': env.ac`tion_space.shape[0],\n",
    "    'actor_activation': 'softmax',\n",
    "    'gamma': 0.99,\n",
    "    'tau': 0.125,\n",
    "    'actor_alpha': 1e-3,\n",
    "    'critic_alpha': 1e-3\n",
    "}\n",
    "model = DDPGModel(base_model, **ddpg_config)\n",
    "\n",
    "schedule = LinearDecay(1.0, 0.1, 500, -1)\n",
    "explorer = EpsilonGreedy(schedule, discrete=False)\n",
    "buffer_size = 10000\n",
    "batch_size = 32\n",
    "\n",
    "agent = Agent(env,\n",
    "              state_processing_fns=[],\n",
    "              model=model, ptoc_fn=lambda x,y: return x,\n",
    "              ctol_fn=lambda x,y: return x,\n",
    "              explorer=explorer, buffer_size=buffer_size,\n",
    "              batch_size=batch_size, warmup_length=0)\n",
    "agent.reset()\n",
    "agent.run_indefinitely()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BREAKOUT\n",
    "\n",
    "# env\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env.reset()\n",
    "\n",
    "# custom shape due to downsampling and stacking\n",
    "shape = (105, 80, 4)\n",
    "\n",
    "# model\n",
    "base_config = {\n",
    "    'in_shape': shape,\n",
    "    'conv_layer_sizes': [16, 32],\n",
    "    'fc_layer_sizes': [256],\n",
    "    'kernel_sizes': [(8,8), (4,4)],\n",
    "    'strides': [(4,4), (2,2)],\n",
    "    'activation': 'relu'\n",
    "}\n",
    "\n",
    "base_model = cnn_model_base(**base_config)\n",
    "\n",
    "dqn_config = {\n",
    "    'action_dim': env.action_space.n,\n",
    "    'gamma': 0.99,\n",
    "    'tau': 0.15,\n",
    "    'optimizer': RMSprop(lr=0.00025, rho=0.95, epsilon=0.01)\n",
    "}\n",
    "model = DQNModel(base_model, **dqn_config)\n",
    "\n",
    "schedule = LinearDecay(1.0, 0.1, 1000000, interval=1)\n",
    "explorer = EpsilonGreedy(schedule)\n",
    "buffer_size = 100000\n",
    "batch_size = 32\n",
    "\n",
    "agent = Agent(env,\n",
    "              state_processing_fns=[downsample(shape[:-1]), rgb_to_binary(),\n",
    "                                    #combine_consecutive(fun='diff'),\n",
    "                                    stack_consecutive(4)],\n",
    "              model=model, ptoc_fn=argmax_scalar(),\n",
    "              ctol_fn=scalar_to_onehot(),\n",
    "              explorer=explorer, buffer_size=buffer_size,\n",
    "              batch_size=batch_size, warmup_length=50000,\n",
    "              state_dtype=np.uint8)\n",
    "agent.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run_indefinitely()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
