{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home')\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore', module='skimage')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from kerasgym.models import cnn_model_base, dense_model_base, DDPGModel, DQNModel\n",
    "from kerasgym.agents import Agent\n",
    "from kerasgym.agents.processing import downsample, rgb_to_binary\n",
    "from kerasgym.agents.processing import stack_consecutive, combine_consecutive\n",
    "from kerasgym.agents.schedules import LinearDecay, ScopingPeriodic\n",
    "from kerasgym.agents.actors import EpsilonGreedyActor\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "\n",
    "from kerasgym.agents.schedules import graph_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "action_type is discrete\n",
      "Action type is discrete\n",
      "Action type is discrete\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DQNModel' object has no attribute 'action_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8f9b15800afe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m agent = Agent(env, state_pipeline=[], model=model, actor=actor, \n\u001b[1;32m     26\u001b[0m               \u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m               warmup_length=0)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#agent.run_indefinitely()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kerasgym/agents/agents.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, state_pipeline, model, actor, buffer_size, batch_size, warmup_length, repeated_actions, report_freq, state_dtype)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         self.replay_buffer = ReplayBuffer(buffer_size, model.in_shape,\n\u001b[0;32m---> 31\u001b[0;31m                                           model.action_dim, state_dtype)\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarmup_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwarmup_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DQNModel' object has no attribute 'action_dim'"
     ]
    }
   ],
   "source": [
    "# CARTPOLE\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "\n",
    "base_config = {\n",
    "    'in_shape': env.observation_space.shape,\n",
    "    'layer_sizes': [64, 32, 16],\n",
    "    'activation': 'relu'\n",
    "}\n",
    "base_model = dense_model_base(**base_config)\n",
    "\n",
    "dqn_config = {\n",
    "    'gamma': 0.99,\n",
    "    'tau': 1.0,\n",
    "    'optimizer': Adam(lr=0.001)\n",
    "}\n",
    "model = DQNModel(base_model, **dqn_config)\n",
    "\n",
    "schedule = LinearDecay(1.0, 0.1, 500, -1)\n",
    "actor = EpsilonGreedyActor(epsilon_schedule=schedule)\n",
    "buffer_size = 10000\n",
    "batch_size = 32\n",
    "\n",
    "agent = Agent(env, state_pipeline=[], model=model, actor=actor, \n",
    "              buffer_size=buffer_size, batch_size=batch_size,\n",
    "              warmup_length=0)\n",
    "agent.reset()\n",
    "#agent.run_indefinitely()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multidiscrete'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kerasgym.util import get_action_type\n",
    "from gym.spaces import MultiDiscrete\n",
    "get_action_type(MultiDiscrete([2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_schedule(ScopingPeriodic(0.9, 0.1, duration=1000), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOUNTAIN CAR\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env.reset()\n",
    "\n",
    "base_config = {\n",
    "    'in_shape': env.observation_space.shape,\n",
    "    'layer_sizes': [16, 16],\n",
    "    'activation': 'relu'\n",
    "}\n",
    "\n",
    "base_model = dense_model_base(**base_config)\n",
    "\n",
    "ddpg_config = {\n",
    "    'action_dim': env.ac`tion_space.shape[0],\n",
    "    'actor_activation': 'softmax',\n",
    "    'gamma': 0.99,\n",
    "    'tau': 0.125,\n",
    "    'actor_alpha': 1e-3,\n",
    "    'critic_alpha': 1e-3\n",
    "}\n",
    "model = DDPGModel(base_model, **ddpg_config)\n",
    "\n",
    "schedule = LinearDecay(1.0, 0.1, 500, -1)\n",
    "explorer = EpsilonGreedy(schedule, discrete=False)\n",
    "buffer_size = 10000\n",
    "batch_size = 32\n",
    "\n",
    "agent = Agent(env,\n",
    "              state_processing_fns=[],\n",
    "              model=model, ptoc_fn=lambda x,y: return x,\n",
    "              ctol_fn=lambda x,y: return x,\n",
    "              explorer=explorer, buffer_size=buffer_size,\n",
    "              batch_size=batch_size, warmup_length=0)\n",
    "agent.reset()\n",
    "agent.run_indefinitely()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BREAKOUT\n",
    "\n",
    "# env\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env.reset()\n",
    "\n",
    "# custom shape due to downsampling and stacking\n",
    "shape = (105, 80, 4)\n",
    "\n",
    "# model\n",
    "base_config = {\n",
    "    'in_shape': shape,\n",
    "    'conv_layer_sizes': [16, 32],\n",
    "    'fc_layer_sizes': [256],\n",
    "    'kernel_sizes': [(8,8), (4,4)],\n",
    "    'strides': [(4,4), (2,2)],\n",
    "    'activation': 'relu'\n",
    "}\n",
    "\n",
    "base_model = cnn_model_base(**base_config)\n",
    "\n",
    "dqn_config = {\n",
    "    'action_dim': env.action_space.n,\n",
    "    'gamma': 0.99,\n",
    "    'tau': 0.15,\n",
    "    'optimizer': RMSprop(lr=0.00025, rho=0.95, epsilon=0.01)\n",
    "}\n",
    "model = DQNModel(base_model, **dqn_config)\n",
    "\n",
    "schedule = LinearDecay(1.0, 0.1, 1000000, interval=1)\n",
    "explorer = EpsilonGreedy(schedule)\n",
    "buffer_size = 100000\n",
    "batch_size = 32\n",
    "\n",
    "agent = Agent(env,\n",
    "              state_processing_fns=[downsample(shape[:-1]), rgb_to_binary(),\n",
    "                                    #combine_consecutive(fun='diff'),\n",
    "                                    stack_consecutive(4)],\n",
    "              model=model, ptoc_fn=argmax_scalar(),\n",
    "              ctol_fn=scalar_to_onehot(),\n",
    "              explorer=explorer, buffer_size=buffer_size,\n",
    "              batch_size=batch_size, warmup_length=50000,\n",
    "              state_dtype=np.uint8)\n",
    "agent.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run_indefinitely()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
